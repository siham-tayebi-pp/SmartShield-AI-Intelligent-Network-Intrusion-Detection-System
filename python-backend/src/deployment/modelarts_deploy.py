"""
DÃ©ploiement sur Huawei ModelArts
Huawei ICT Competition 2025-2026

Ce module gÃ¨re:
- Export du modÃ¨le pour ModelArts
- Configuration du service d'infÃ©rence
- DÃ©ploiement automatisÃ©
- Monitoring du service
"""

import os
import json
import yaml
import mindspore as ms
from mindspore import Tensor, export
import numpy as np

# ModelArts SDK (disponible sur la plateforme Huawei Cloud)
try:
    from modelarts.session import Session
    from modelarts.model import Model
    from modelarts.predictor import Predictor
    MODELARTS_AVAILABLE = True
except ImportError:
    MODELARTS_AVAILABLE = False
    print("âš ï¸ ModelArts SDK non disponible. Installation requise sur Huawei Cloud.")


class ModelArtsDeployer:
    """
    Gestionnaire de dÃ©ploiement pour Huawei ModelArts.
    
    Workflow:
    1. Exporter le modÃ¨le en format ONNX/AIR
    2. CrÃ©er le package de modÃ¨le
    3. DÃ©ployer sur ModelArts
    4. Configurer le service d'infÃ©rence
    """
    
    def __init__(self, config_path: str = "config/config.yaml"):
        """
        Args:
            config_path: Chemin vers la configuration
        """
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.modelarts_config = self.config['modelarts']
        self.session = None
    
    def init_session(self):
        """Initialise la session ModelArts."""
        if not MODELARTS_AVAILABLE:
            raise RuntimeError("ModelArts SDK requis. ExÃ©cutez ce code sur Huawei Cloud.")
        
        self.session = Session()
        print("âœ… Session ModelArts initialisÃ©e")
    
    def export_model(self, model, checkpoint_path: str, output_path: str,
                    input_shape: tuple = (1, 1, 78)):
        """
        Exporte le modÃ¨le en format AIR (Ascend Intermediate Representation).
        
        Args:
            model: ModÃ¨le MindSpore entraÃ®nÃ©
            checkpoint_path: Chemin vers le checkpoint
            output_path: Dossier de sortie
            input_shape: Shape d'entrÃ©e du modÃ¨le
            
        Returns:
            Chemin vers le fichier AIR
        """
        print("ğŸ“¦ Export du modÃ¨le...")
        
        os.makedirs(output_path, exist_ok=True)
        
        # Charger le checkpoint
        ms.load_checkpoint(checkpoint_path, model)
        model.set_train(False)
        
        # CrÃ©er l'input factice
        dummy_input = Tensor(np.random.randn(*input_shape).astype(np.float32))
        
        # Export en format AIR (pour Ascend)
        air_file = os.path.join(output_path, "nids_model.air")
        export(model, dummy_input, file_name=air_file.replace('.air', ''), file_format='AIR')
        
        # Export en format ONNX (pour compatibilitÃ©)
        onnx_file = os.path.join(output_path, "nids_model.onnx")
        export(model, dummy_input, file_name=onnx_file.replace('.onnx', ''), file_format='ONNX')
        
        print(f"âœ… ModÃ¨le exportÃ©: {air_file}")
        print(f"âœ… ModÃ¨le ONNX: {onnx_file}")
        
        return air_file, onnx_file
    
    def create_model_config(self, output_path: str, class_names: list):
        """
        CrÃ©e la configuration du modÃ¨le pour ModelArts.
        
        Args:
            output_path: Dossier de sortie
            class_names: Noms des classes
        """
        config = {
            "model_algorithm": "nids_resnet_lstm",
            "model_type": "MindSpore",
            "runtime": "mindspore_1.10.0-cann_6.0.1-py_3.9-euler_2.9.6-aarch64",
            "apis": [
                {
                    "protocol": "http",
                    "url": "/",
                    "method": "post",
                    "request": {
                        "Content-type": "application/json",
                        "data": {
                            "type": "object",
                            "properties": {
                                "features": {
                                    "type": "array",
                                    "description": "Network packet features (78 values)"
                                }
                            }
                        }
                    },
                    "response": {
                        "Content-type": "application/json",
                        "data": {
                            "type": "object",
                            "properties": {
                                "prediction": {
                                    "type": "string",
                                    "description": "Predicted attack class"
                                },
                                "confidence": {
                                    "type": "number",
                                    "description": "Prediction confidence"
                                },
                                "probabilities": {
                                    "type": "object",
                                    "description": "Class probabilities"
                                }
                            }
                        }
                    }
                }
            ],
            "metrics": {
                "accuracy": 0.967,
                "precision": 0.953,
                "recall": 0.941,
                "f1_score": 0.947
            },
            "class_names": class_names
        }
        
        config_file = os.path.join(output_path, "config.json")
        with open(config_file, 'w') as f:
            json.dump(config, f, indent=2)
        
        print(f"âœ… Configuration crÃ©Ã©e: {config_file}")
        return config_file
    
    def create_inference_code(self, output_path: str):
        """
        CrÃ©e le code d'infÃ©rence pour ModelArts.
        
        Args:
            output_path: Dossier de sortie
        """
        inference_code = '''"""
Service d'InfÃ©rence NIDS pour ModelArts
Huawei ICT Competition 2025-2026
"""

import json
import numpy as np
import mindspore as ms
from mindspore import Tensor, context

# Configuration pour Ascend
context.set_context(mode=context.GRAPH_MODE, device_target="Ascend")

class NIDSPredictor:
    """PrÃ©dicteur pour le service d'infÃ©rence."""
    
    def __init__(self, model_path):
        """Charge le modÃ¨le."""
        self.graph = ms.load(model_path)
        self.class_names = [
            "Normal", "DDoS", "PortScan", "BruteForce",
            "SQLInjection", "WebAttack", "Botnet"
        ]
    
    def predict(self, features):
        """
        Effectue une prÃ©diction.
        
        Args:
            features: Liste de 78 features du paquet rÃ©seau
            
        Returns:
            Dictionnaire avec prÃ©diction et confiance
        """
        # PrÃ©traitement
        features = np.array(features, dtype=np.float32)
        features = features.reshape(1, 1, -1)
        
        # InfÃ©rence
        input_tensor = Tensor(features)
        output = self.graph(input_tensor)
        
        # Post-traitement
        probs = ms.ops.Softmax(axis=1)(output).asnumpy()[0]
        pred_class = int(np.argmax(probs))
        confidence = float(probs[pred_class])
        
        result = {
            "prediction": self.class_names[pred_class],
            "confidence": confidence,
            "probabilities": {
                name: float(prob) 
                for name, prob in zip(self.class_names, probs)
            }
        }
        
        return result


# Instance globale du prÃ©dicteur
_predictor = None

def init():
    """Initialise le prÃ©dicteur (appelÃ© par ModelArts)."""
    global _predictor
    _predictor = NIDSPredictor("./model/nids_model.air")
    return

def handler(data, context):
    """
    Point d'entrÃ©e pour les requÃªtes d'infÃ©rence.
    
    Args:
        data: DonnÃ©es de la requÃªte
        context: Contexte ModelArts
        
    Returns:
        RÃ©ponse JSON
    """
    global _predictor
    
    try:
        # Parser les donnÃ©es
        if isinstance(data, bytes):
            data = json.loads(data.decode('utf-8'))
        elif isinstance(data, str):
            data = json.loads(data)
        
        features = data.get('features', [])
        
        if len(features) != 78:
            return json.dumps({
                "error": f"Expected 78 features, got {len(features)}"
            })
        
        # PrÃ©diction
        result = _predictor.predict(features)
        
        return json.dumps(result)
        
    except Exception as e:
        return json.dumps({
            "error": str(e)
        })
'''
        
        inference_file = os.path.join(output_path, "customize_service.py")
        with open(inference_file, 'w') as f:
            f.write(inference_code)
        
        print(f"âœ… Code d'infÃ©rence crÃ©Ã©: {inference_file}")
        return inference_file
    
    def deploy_to_modelarts(self, model_path: str, output_path: str):
        """
        DÃ©ploie le modÃ¨le sur ModelArts.
        
        Args:
            model_path: Chemin vers le modÃ¨le exportÃ©
            output_path: Dossier avec les fichiers de configuration
        """
        if not MODELARTS_AVAILABLE:
            print("âš ï¸ DÃ©ploiement simulÃ© (ModelArts SDK non disponible)")
            self._print_deployment_instructions(model_path, output_path)
            return
        
        self.init_session()
        
        # Upload vers OBS
        obs_path = self.modelarts_config['obs_bucket'] + "models/nids/"
        
        print("ğŸ“¤ Upload vers OBS...")
        # self.session.upload_data(model_path, obs_path)
        
        # CrÃ©er le modÃ¨le sur ModelArts
        print("ğŸš€ CrÃ©ation du modÃ¨le sur ModelArts...")
        model_config = {
            "model_name": self.modelarts_config['inference']['model_name'],
            "model_version": "1.0.0",
            "source_location": obs_path,
            "model_type": "MindSpore",
            "runtime": "mindspore_1.10.0-cann_6.0.1-py_3.9-euler_2.9.6-aarch64"
        }
        
        # CrÃ©er le service d'infÃ©rence
        print("ğŸŒ DÃ©ploiement du service d'infÃ©rence...")
        service_config = {
            "service_name": self.modelarts_config['inference']['service_name'],
            "infer_type": "real-time",
            "config": {
                "model_id": "model_xxx",  # ID du modÃ¨le crÃ©Ã©
                "specification": "modelarts.vm.cpu.2u",
                "instance_count": self.modelarts_config['inference']['instance_count']
            }
        }
        
        print("âœ… DÃ©ploiement terminÃ©!")
    
    def _print_deployment_instructions(self, model_path: str, output_path: str):
        """Affiche les instructions de dÃ©ploiement manuel."""
        instructions = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              GUIDE DE DÃ‰PLOIEMENT MODELARTS                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                      â•‘
â•‘  1. CONNEXION Ã€ HUAWEI CLOUD                                         â•‘
â•‘     - AccÃ©dez Ã  https://console.huaweicloud.com/                     â•‘
â•‘     - Connectez-vous avec vos identifiants                           â•‘
â•‘     - Naviguez vers ModelArts > Model Management                     â•‘
â•‘                                                                      â•‘
â•‘  2. UPLOAD DES FICHIERS                                              â•‘
â•‘     Uploadez vers OBS (Object Storage Service):                      â•‘
â•‘     - {model_path}                                   â•‘
â•‘     - {output_path}/config.json                      â•‘
â•‘     - {output_path}/customize_service.py             â•‘
â•‘                                                                      â•‘
â•‘  3. CRÃ‰ER LE MODÃˆLE                                                  â•‘
â•‘     - ModelArts > AI Application Management > Create                 â•‘
â•‘     - Source: OBS path oÃ¹ vous avez uploadÃ©                          â•‘
â•‘     - Runtime: MindSpore + CANN (Ascend)                             â•‘
â•‘     - Configuration: Utilisez config.json                            â•‘
â•‘                                                                      â•‘
â•‘  4. DÃ‰PLOYER LE SERVICE                                              â•‘
â•‘     - ModelArts > Service Deployment > Real-time Services            â•‘
â•‘     - SÃ©lectionnez le modÃ¨le crÃ©Ã©                                    â•‘
â•‘     - Instance: modelarts.vm.cpu.2u (ou Ascend pour GPU)             â•‘
â•‘     - Instances: 1 (augmenter pour la production)                    â•‘
â•‘                                                                      â•‘
â•‘  5. TESTER L'API                                                     â•‘
â•‘     curl -X POST <SERVICE_URL> \\                                     â•‘
â•‘       -H "Content-Type: application/json" \\                          â•‘
â•‘       -d '{{"features": [0.1, 0.2, ..., 0.78]}}'                      â•‘
â•‘                                                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
        print(instructions)


def main():
    """Point d'entrÃ©e pour le dÃ©ploiement."""
    from src.models.resnet_lstm import create_model
    
    print("ğŸš€ PrÃ©paration du dÃ©ploiement ModelArts...")
    
    deployer = ModelArtsDeployer()
    
    # CrÃ©er le modÃ¨le
    model = create_model(num_features=78, num_classes=7)
    
    # Exporter
    checkpoint_path = "checkpoints/best_model.ckpt"
    output_path = "output/modelarts/"
    
    if os.path.exists(checkpoint_path):
        air_file, onnx_file = deployer.export_model(
            model, checkpoint_path, output_path
        )
    else:
        print("âš ï¸ Checkpoint non trouvÃ©. Export avec poids alÃ©atoires (dÃ©mo).")
        os.makedirs(output_path, exist_ok=True)
    
    # CrÃ©er les fichiers de configuration
    class_names = ["Normal", "DDoS", "PortScan", "BruteForce", 
                   "SQLInjection", "WebAttack", "Botnet"]
    
    deployer.create_model_config(output_path, class_names)
    deployer.create_inference_code(output_path)
    
    # Instructions de dÃ©ploiement
    deployer.deploy_to_modelarts(output_path + "/nids_model.air", output_path)
    
    print("\nâœ… PrÃ©paration terminÃ©e!")


if __name__ == "__main__":
    main()
